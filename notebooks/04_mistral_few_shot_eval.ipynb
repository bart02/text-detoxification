{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rUouJzdfLfOl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-05 13:09:13--  https://github.com/skoltech-nlp/detox/releases/download/emnlp2021/filtered_paranmt.zip\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://github.com/s-nlp/detox/releases/download/emnlp2021/filtered_paranmt.zip [following]\n",
      "--2023-11-05 13:09:13--  https://github.com/s-nlp/detox/releases/download/emnlp2021/filtered_paranmt.zip\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/402743074/ea18dc6d-ab2d-49da-9cd3-2903867da5d3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231105%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231105T130913Z&X-Amz-Expires=300&X-Amz-Signature=7a17575bdfcaf70d8b2ab7c4cf09afa2dd3e473baccb9f08ac8c893f93a583ef&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=402743074&response-content-disposition=attachment%3B%20filename%3Dfiltered_paranmt.zip&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-11-05 13:09:13--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/402743074/ea18dc6d-ab2d-49da-9cd3-2903867da5d3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231105%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231105T130913Z&X-Amz-Expires=300&X-Amz-Signature=7a17575bdfcaf70d8b2ab7c4cf09afa2dd3e473baccb9f08ac8c893f93a583ef&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=402743074&response-content-disposition=attachment%3B%20filename%3Dfiltered_paranmt.zip&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 44376072 (42M) [application/octet-stream]\n",
      "Saving to: ‘filtered_paranmt.zip.1’\n",
      "\n",
      "filtered_paranmt.zi 100%[===================>]  42.32M  49.8MB/s    in 0.9s    \n",
      "\n",
      "2023-11-05 13:09:14 (49.8 MB/s) - ‘filtered_paranmt.zip.1’ saved [44376072/44376072]\n",
      "\n",
      "Archive:  filtered_paranmt.zip\n",
      "  inflating: filtered.tsv            \n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/skoltech-nlp/detox/releases/download/emnlp2021/filtered_paranmt.zip\n",
    "!unzip filtered_paranmt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34febaa06d804e81a1b3ef17236c731f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", device_map=\"auto\", torch_dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rUouJzdfLfOl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"filtered.tsv\", sep=\"\\t\", index_col='Unnamed: 0')\n",
    "df = df.drop(columns=['similarity', 'lenght_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aZYCAl6HW0xJ"
   },
   "outputs": [],
   "source": [
    "cond = df[\"trn_tox\"] > df[\"ref_tox\"]\n",
    "df.loc[cond, [\"reference\", \"translation\"]] = df.loc[cond, [\"translation\", \"reference\"]].values\n",
    "df.loc[cond, [\"ref_tox\", \"trn_tox\"]] = df.loc[cond, [\"trn_tox\", \"ref_tox\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4lM_wCrebzBW"
   },
   "outputs": [],
   "source": [
    "df = df[((df[\"ref_tox\"] > 0.8) & (df[\"trn_tox\"] < 0.1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "slmjVgN_cYAe"
   },
   "outputs": [],
   "source": [
    "df = df[((df['reference'].str.len() > 30) & (df['translation'].str.len() > 20))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values(by=['ref_tox', 'trn_tox'],\n",
    "                           key=lambda x: abs(df['ref_tox'] - df['trn_tox']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Kuf-ajnmew2n"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(sorted_df, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "id": "NTt55TledRZq"
   },
   "outputs": [],
   "source": [
    "for_prompt_high = train[-5000:].sample(10)\n",
    "\n",
    "med = len(sorted_df) // 2\n",
    "for_prompt_medium = train[(med-2500):(med+2500)].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "EWo-e-_IeJek",
    "outputId": "e3b99e21-9ea3-46cc-a7ca-75cc80cd9fd2"
   },
   "outputs": [],
   "source": [
    "for_prompt = pd.concat([for_prompt_high, for_prompt_medium])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "id": "1tS5kkard68l"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt = '[INST] Make text NON TOXIC according to the examples. Write ONLY Non-toxic text as an output! Try to make it as similar as possible to the original text. It could not be no response. [/INST] \\n'\n",
    "for _, e in for_prompt.iterrows():\n",
    "    prompt += f\" [INST] Toxic text: {e['reference']}\\nNon-toxic text: [/INST] {e['translation']}\\n\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_test = test[:1000].sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "\n",
    "for _, e in for_test.iterrows():\n",
    "    prompts.append(f\"{prompt} [INST] Toxic text: {e['reference']}\\nNon-toxic text: [/INST] \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e37edce059544a5b3960840e4e5e398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "gen = []\n",
    "\n",
    "for inp in tqdm(prompts):\n",
    "    encodeds = tokenizer(inp, return_tensors=\"pt\")\n",
    "    model_inputs = encodeds.to(device)\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True, eos_token_id=[2, 13], temperature=1, repetition_penalty=1.2)\n",
    "    decoded = tokenizer.batch_decode(generated_ids[:, model_inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    gen.append(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_indexes = []\n",
    "new_gen = []\n",
    "\n",
    "for i, e in enumerate(gen):\n",
    "    q = ''.join([s for s in e if s.isalpha() or s == ' '])\n",
    "    q = q.strip()\n",
    "    if len(q) < 5:\n",
    "        removed_indexes.append(i)\n",
    "    else:\n",
    "        new_gen.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = [e for i, e in enumerate(for_test[\"reference\"]) if i not in removed_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "r_tokenizer = RobertaTokenizer.from_pretrained('SkolkovoInstitute/roberta_toxicity_classifier')\n",
    "r_model = RobertaForSequenceClassification.from_pretrained('SkolkovoInstitute/roberta_toxicity_classifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def classify_preds_toxicity(preds, batch_size=16):\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(0, len(preds), batch_size)):\n",
    "        batch = r_tokenizer(preds[i:i + batch_size], return_tensors='pt', padding=True)\n",
    "        result = (r_model(**batch)['logits'] / 2.5).softmax(dim=1)[:,1].data.tolist()\n",
    "        results.extend([1 - item for item in result])\n",
    "\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9996d815054b8ab1c1639329fe3d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "non_toxixty = classify_preds_toxicity(new_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def calc_bleu(inputs, preds):\n",
    "    results = []\n",
    "    \n",
    "    print('Calculating BLEU similarity')\n",
    "    for i in range(len(inputs)):\n",
    "        results.append(sentence_bleu([inputs[i]], preds[i]))\n",
    "\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BLEU similarity\n"
     ]
    }
   ],
   "source": [
    "bleu = calc_bleu(reference, new_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.embeddings import FlairEmbeddings\n",
    "from torch.nn.functional import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flair_sim(inputs, preds, batch_size = 16):\n",
    "    print('Calculating flair embeddings similarity')\n",
    "    sim = 0\n",
    "    batch_size = 16\n",
    "    inp_embed = []\n",
    "    pred_embed = []\n",
    "\n",
    "    embedder = FlairEmbeddings('news-forward')\n",
    "\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        inp_part = [Sentence(sent) for sent in inputs[i:i + batch_size]]\n",
    "        pred_part = [Sentence(sent) for sent in preds[i:i + batch_size]]\n",
    "\n",
    "        inp_part = embedder.embed(inp_part)\n",
    "        pred_part = embedder.embed(pred_part)\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            if ((i + j) < len(inputs)):\n",
    "                inp_sent_vec = torch.zeros(2048).cuda()\n",
    "                pred_sent_vec = torch.zeros(2048).cuda()\n",
    "\n",
    "                for k in range(len(inp_part[j])):\n",
    "                    inp_sent_vec += inp_part[j][k].embedding\n",
    "                inp_embed.append(inp_sent_vec.cpu() / (k + 1))\n",
    "\n",
    "                for k in range(len(pred_part[j])):\n",
    "                    pred_sent_vec += pred_part[j][k].embedding\n",
    "                pred_embed.append(pred_sent_vec.cpu() / (k + 1))\n",
    "\n",
    "    emb_sim = cosine_similarity(torch.stack(inp_embed), torch.stack(pred_embed))\n",
    "\n",
    "    return emb_sim.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating flair embeddings similarity\n"
     ]
    }
   ],
   "source": [
    "emb_sim = flair_sim(reference, new_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-CoLA were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "cola_tokenizer = AutoTokenizer.from_pretrained(\"textattack/roberta-base-CoLA\")\n",
    "cola_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/roberta-base-CoLA\")\n",
    "\n",
    "\n",
    "def classify_cola(preds, batch_size=16):\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(0, len(preds), batch_size)):\n",
    "        batch = cola_tokenizer(preds[i:i + batch_size], return_tensors='pt', padding=True)\n",
    "        result = (cola_model(**batch)['logits']).softmax(dim=1)[:,1].data.tolist()\n",
    "        results.extend(result)\n",
    "\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836a707f3af044c5860050fb2241bfae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cola = classify_cola(new_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = non_toxixty * np.mean([bleu, emb_sim], axis=0) * cola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13781250251061722"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_score.sum() / 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
