{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rUouJzdfLfOl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"filtered.tsv\", sep=\"\\t\", index_col='Unnamed: 0')\n",
    "df = df.drop(columns=['similarity', 'lenght_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aZYCAl6HW0xJ"
   },
   "outputs": [],
   "source": [
    "cond = df[\"trn_tox\"] > df[\"ref_tox\"]\n",
    "df.loc[cond, [\"reference\", \"translation\"]] = df.loc[cond, [\"translation\", \"reference\"]].values\n",
    "df.loc[cond, [\"ref_tox\", \"trn_tox\"]] = df.loc[cond, [\"trn_tox\", \"ref_tox\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4lM_wCrebzBW"
   },
   "outputs": [],
   "source": [
    "df = df[((df[\"ref_tox\"] > 0.8) & (df[\"trn_tox\"] < 0.1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "slmjVgN_cYAe"
   },
   "outputs": [],
   "source": [
    "df = df[((df['reference'].str.len() > 30) & (df['translation'].str.len() > 20))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.5854577740249"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reference'].str.len().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values(by=['ref_tox', 'trn_tox'],\n",
    "                           key=lambda x: abs(df['ref_tox'] - df['trn_tox']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Kuf-ajnmew2n"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(sorted_df, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0648e459746d42d889dc06a0cead7edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a857b8c275740ff865112ccab0c0e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3029274b3b49d8a72fe7d9f9a697e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696071ebe90c4f1aa7458ca7a9fa7eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a38468b60149b5a6df9fb66167c05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa7cbf96eb34282b6d8c220d92c9a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef959a5cddea4fa38943644a12131476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFFIX = \"detoxify: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df.drop(columns=['ref_tox', 'trn_tox']), preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['reference', 'translation'],\n",
       "    num_rows: 322124\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = dataset.train_test_split(test_size=0.05)\n",
    "\n",
    "# Take the test part and split it further into validation and test datasets (50-50 split)\n",
    "validation_test_split = train_test_split['test'].train_test_split(test_size=0.8)\n",
    "\n",
    "# Now, construct a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': validation_test_split['train'],\n",
    "    'test': validation_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['reference', 'translation'],\n",
       "        num_rows: 306017\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['reference', 'translation'],\n",
       "        num_rows: 3221\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['reference', 'translation'],\n",
       "        num_rows: 12886\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    inputs = [PREFFIX + e for e in examples[\"reference\"]]\n",
    "    targets = examples[\"translation\"]\n",
    "    \n",
    "    tokenized_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "    tokenized_targets = tokenizer(targets, max_length=128, truncation=True)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = tokenized_targets[\"input_ids\"]\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f837d0ec5e9940f9b56ad0cf8a7b61e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/306017 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68105e07f72749779cef04ff3515b3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe8921cd41d42ea9548b16b7ae93670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12886 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize, batched=True, remove_columns=['reference', 'translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 306017\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3221\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 12886\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2ca3563f1a4d2183d3a7c3e683bcaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2593742afc8f49299012c947b9983205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235d571d485d4775a808afbc293cee11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    print(result)\n",
    "    result = {\"bleu\": result[\"bleu\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gconf = model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gconf.max_new_tokens = 128\n",
    "gconf.repetition_penalty = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"t5_detox\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=2000,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=200,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    generation_config=gconf,\n",
    "    report_to='wandb',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbatalov\u001b[0m (\u001b[33mcontextr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20231105_173013-9l3nxy3q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/contextr/huggingface/runs/9l3nxy3q' target=\"_blank\">peachy-dream-7</a></strong> to <a href='https://wandb.ai/contextr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/contextr/huggingface' target=\"_blank\">https://wandb.ai/contextr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/contextr/huggingface/runs/9l3nxy3q' target=\"_blank\">https://wandb.ai/contextr/huggingface/runs/9l3nxy3q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12827' max='38256' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12827/38256 36:45 < 1:12:53, 5.81 it/s, Epoch 1.34/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.940200</td>\n",
       "      <td>1.741413</td>\n",
       "      <td>0.259700</td>\n",
       "      <td>18.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.850300</td>\n",
       "      <td>1.691743</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>18.365700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.834200</td>\n",
       "      <td>1.662268</td>\n",
       "      <td>0.269500</td>\n",
       "      <td>18.236600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.820100</td>\n",
       "      <td>1.643432</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>18.254900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.756700</td>\n",
       "      <td>1.629483</td>\n",
       "      <td>0.274400</td>\n",
       "      <td>18.198100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.745000</td>\n",
       "      <td>1.619735</td>\n",
       "      <td>0.274700</td>\n",
       "      <td>18.189700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.25969010429550515, 'precisions': [0.5902148878602893, 0.3337349397590361, 0.20875675958097129, 0.13086884591248787], 'brevity_penalty': 0.9588115052909236, 'length_ratio': 0.9596369254538432, 'translation_length': 44721, 'reference_length': 46602}\n",
      "{'bleu': 0.26709993891652395, 'precisions': [0.5962205076596221, 0.3420735528028149, 0.21597470801870772, 0.13679675910076458], 'brevity_penalty': 0.9586774469196508, 'length_ratio': 0.9595081756147805, 'translation_length': 44715, 'reference_length': 46602}\n",
      "{'bleu': 0.2694854262314393, 'precisions': [0.6006705368795284, 0.34683777686130857, 0.21976315789473685, 0.13991201587164667], 'brevity_penalty': 0.9525595319999517, 'length_ratio': 0.9536500579374276, 'translation_length': 44442, 'reference_length': 46602}\n",
      "{'bleu': 0.272086100396631, 'precisions': [0.6027754661388632, 0.34963627546071774, 0.2223625029590468, 0.1417897580320708], 'brevity_penalty': 0.9529864797919304, 'length_ratio': 0.9540577657611261, 'translation_length': 44461, 'reference_length': 46602}\n",
      "{'bleu': 0.27439242934779406, 'precisions': [0.6055606959897091, 0.3527378924312485, 0.2251181705352663, 0.14497229277303164], 'brevity_penalty': 0.9496111086922449, 'length_ratio': 0.9508390197845586, 'translation_length': 44311, 'reference_length': 46602}\n",
      "{'bleu': 0.27465228091732036, 'precisions': [0.6053496645201963, 0.35226858932342875, 0.22495523016959865, 0.14445627463957872], 'brevity_penalty': 0.9519300291673022, 'length_ratio': 0.9530492253551349, 'translation_length': 44414, 'reference_length': 46602}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.sample(range(len(dataset_dict[\"test\"])), k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_test = [dataset_dict[\"test\"][i] for i in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps = [e[\"reference\"] for e in for_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9925136a14134120b7e6dbf2aee9eefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen = []\n",
    "\n",
    "for inp in tqdm(inps):\n",
    "    encodeds = tokenizer(PREFFIX + inp, return_tensors=\"pt\")\n",
    "    model_inputs = encodeds.to('cuda')\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=128)\n",
    "    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    gen.append(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If anyone else is, you should think you screwed up the Rockettes.',\n",
       " '\"We should have stuffed him in the hole behind his friends,\" said Dextrinos.',\n",
       " \"Uh, yeah, I'm really sorry about that.\",\n",
       " \"and I'm going crazy because I don't.\",\n",
       " 'Stephen Morton shot and killed 15 people.',\n",
       " \"why didn't you just cut me out when you had the chance?\",\n",
       " \"No, we shouldn't tell her because she's going to go crazy.\",\n",
       " 'what are they gonna do?',\n",
       " 'The black men shot at me as a target, and the dogs tried to tear me apart.',\n",
       " \"What's the matter with you?\",\n",
       " \"you won't throw it in the trash can when you leave?\",\n",
       " \"but he's got Red's money.\",\n",
       " \"I'm still bringing her to jail, okay?\",\n",
       " \"we moved two savages into the adjacent room, and now we can't sleep.\",\n",
       " 'who cares about being friends with Jane and Brad, if we can be friends with Max and Grant?',\n",
       " 'who asked you anything?',\n",
       " \"But if she didn't want it, it would be silly.\",\n",
       " 'you make me look crazy in front of my family!',\n",
       " \"Only white people don't go to Kennebunkport.\",\n",
       " \"why didn't you say anything?\",\n",
       " \"In your useless hands, it's just a rock.\",\n",
       " \"You know what they say: They don't care about any of them.\",\n",
       " 'I said, \"Hell, Lou, wait till we get to Frisco.\"',\n",
       " \"Listen, if you hit me, I'll hit you too!\",\n",
       " 'the priests and vampires are gonna engage angels two, Jesus\\' lhrist - an obvious expletive, not a coder - \"they\\'re at two ldicks\"',\n",
       " 'they have long wooden sticks that they put in your urethra.',\n",
       " 'first, crush Camellia Japonica.',\n",
       " \"Wow, I don't want to scare you, but you just ripped me off.\",\n",
       " \"When people realize that Italian sergeants and Irish cops are bullying their fellow citizens and killing people in places where they don't have jurisdiction, they will riot.\",\n",
       " \"She's completely infected you with her stuff.\",\n",
       " 'You probably think your wife should have died.',\n",
       " '\"pilot, I\\'m going to bathe and wash for you, for a day, but I\\'ll be damn happy if I wear a bloody nightgown!\"',\n",
       " \"I'm a lover of good spirit, and I like animals to die.\",\n",
       " 'I was drinking all night... the bar staff saw me.',\n",
       " \"When you said panic, I didn't know you were gonna hang me.\",\n",
       " \"isn't it enough that English pirates attack and rob our ships, but we're supposed to be crazy in this court?\",\n",
       " 'I know it, you know it, your brother knows it.',\n",
       " '\"You\\'re a dirty businessman, Ekle.',\n",
       " 'and I just got humiliated by the city council, Doug Wilson.',\n",
       " 'He wants to rip my eyes out, men!',\n",
       " 'I\\'m Ratko, the Bosnian drogue, and all around me is a \"black-and-white world...\"',\n",
       " \"all right, let me simplify this for you... every half-hour, I'm shooting another hostage.\",\n",
       " 'and this slap is from your dead father.',\n",
       " \"you make it sound like I'm anaesthetic or something.\",\n",
       " \"When's your wormhole open?\",\n",
       " '\"You know, Carol,\" he returned her list, \"never, even in the wildest years of my imagination, I would have imagined such a complicated and nonsense plot.',\n",
       " \"if she comes home from a hard day's work and finds a bunch of gangsters in her kitchen, they're doing a lot of gangster stuff... there's no way to tell what she's gonna do.\",\n",
       " 'I expect the monkey to be the first in line again.',\n",
       " \"Okay, I learned my lesson. Now I'm a snitch.\",\n",
       " \"That doesn't sound like the usual, mindless, boring, nonsense.\"]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed_indexes = []\n",
    "# new_gen = []\n",
    "\n",
    "# for i, e in enumerate(gen):\n",
    "#     q = ''.join([s for s in e if s.isalpha() or s == ' '])\n",
    "#     q = q.strip()\n",
    "#     if len(q) < 5:\n",
    "#         removed_indexes.append(i)\n",
    "#     else:\n",
    "#         new_gen.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gen = gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "r_tokenizer = RobertaTokenizer.from_pretrained('SkolkovoInstitute/roberta_toxicity_classifier')\n",
    "r_model = RobertaForSequenceClassification.from_pretrained('SkolkovoInstitute/roberta_toxicity_classifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def classify_preds_toxicity(preds, batch_size=16):\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(0, len(preds), batch_size)):\n",
    "        batch = r_tokenizer(preds[i:i + batch_size], return_tensors='pt', padding=True)\n",
    "        result = (r_model(**batch)['logits'] / 2.5).softmax(dim=1)[:,1].data.tolist()\n",
    "        results.extend([1 - item for item in result])\n",
    "\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8492400537cd41e2be678af402e1c8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "non_toxixty = classify_preds_toxicity(new_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def calc_bleu(inputs, preds):\n",
    "    results = []\n",
    "    \n",
    "    print('Calculating BLEU similarity')\n",
    "    for i in range(len(inputs)):\n",
    "        results.append(sentence_bleu([inputs[i]], preds[i]))\n",
    "\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BLEU similarity\n"
     ]
    }
   ],
   "source": [
    "bleu = calc_bleu(inps, new_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.embeddings import FlairEmbeddings\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flair_sim(inputs, preds, batch_size = 16):\n",
    "    print('Calculating flair embeddings similarity')\n",
    "    sim = 0\n",
    "    batch_size = 16\n",
    "    inp_embed = []\n",
    "    pred_embed = []\n",
    "\n",
    "    embedder = FlairEmbeddings('news-forward')\n",
    "\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        inp_part = [Sentence(sent) for sent in inputs[i:i + batch_size]]\n",
    "        pred_part = [Sentence(sent) for sent in preds[i:i + batch_size]]\n",
    "\n",
    "        inp_part = embedder.embed(inp_part)\n",
    "        pred_part = embedder.embed(pred_part)\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            if ((i + j) < len(inputs)):\n",
    "                inp_sent_vec = torch.zeros(2048).cuda()\n",
    "                pred_sent_vec = torch.zeros(2048).cuda()\n",
    "\n",
    "                for k in range(len(inp_part[j])):\n",
    "                    inp_sent_vec += inp_part[j][k].embedding\n",
    "                inp_embed.append(inp_sent_vec.cpu() / (k + 1))\n",
    "\n",
    "                for k in range(len(pred_part[j])):\n",
    "                    pred_sent_vec += pred_part[j][k].embedding\n",
    "                pred_embed.append(pred_sent_vec.cpu() / (k + 1))\n",
    "\n",
    "    emb_sim = cosine_similarity(torch.stack(inp_embed), torch.stack(pred_embed))\n",
    "\n",
    "    return emb_sim.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating flair embeddings similarity\n"
     ]
    }
   ],
   "source": [
    "emb_sim = flair_sim(inps, new_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-CoLA were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "cola_tokenizer = AutoTokenizer.from_pretrained(\"textattack/roberta-base-CoLA\")\n",
    "cola_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/roberta-base-CoLA\")\n",
    "\n",
    "\n",
    "def classify_cola(preds, batch_size=16):\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(0, len(preds), batch_size)):\n",
    "        batch = cola_tokenizer(preds[i:i + batch_size], return_tensors='pt', padding=True)\n",
    "        result = (cola_model(**batch)['logits']).softmax(dim=1)[:,1].data.tolist()\n",
    "        results.extend(result)\n",
    "\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4552e0913abc45dfa439a0c6bdfa97ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cola = classify_cola(new_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = non_toxixty * np.mean([bleu, emb_sim], axis=0) * cola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45200450512875245"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_score.sum() / 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
