1. A. Vaswani <i>et al.</i>, &#x201C;Attention is All you Need,&#x201D; in <i>Advances in Neural Information Processing Systems</i>, Curran Associates, Inc., 2017. Available: https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. [Accessed: Oct. 30, 2023]

2. D. Dale <i>et al.</i>, &#x201C;Text Detoxification using Large Pre-trained Neural Models,&#x201D; in <i>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</i>, M.-F. Moens, X. Huang, L. Specia, and S. W. Yih, Eds., Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 7979&#x2013;7996. doi: 10.18653/v1/2021.emnlp-main.629. Available: https://aclanthology.org/2021.emnlp-main.629. [Accessed: Nov. 05, 2023]

3. C. Raffel <i>et al.</i>, &#x201C;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.&#x201D; arXiv, Sep. 19, 2023. doi: 10.48550/arXiv.1910.10683. Available: http://arxiv.org/abs/1910.10683. [Accessed: Nov. 05, 2023]

3. V. Logacheva <i>et al.</i>, &#x201C;ParaDetox: Detoxification with Parallel Data,&#x201D; in <i>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>, S. Muresan, P. Nakov, and A. Villavicencio, Eds., Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 6804&#x2013;6818. doi: 10.18653/v1/2022.acl-long.469. Available: https://aclanthology.org/2022.acl-long.469. [Accessed: Nov. 05, 2023]

4. K. Krishna, J. Wieting, and M. Iyyer, &#x201C;Reformulating Unsupervised Style Transfer as Paraphrase Generation.&#x201D; arXiv, Oct. 12, 2020. Available: http://arxiv.org/abs/2010.05700. [Accessed: Nov. 05, 2023]

5. Web page, Style Transfer Evaluation. Available: https://github.com/martiansideofthemoon/style-transfer-paraphrase/blob/master/style_paraphrase/evaluation/README.md. [Accessed: Nov. 05, 2023]